{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I implemented three models, which are subtractor, adder-subtractor, adder-subtractor-multiplier, based on `Addition.ipynb` provided in homework3.\n",
    "And I wrote three jupyter notebooks of these models, which named `subtractor.ipynb`, `addition-subtractor.ipynb` and `multiply.ipynb`. (Surely you can check out them)\n",
    "To leverage different configurations to compare performances of the same model, I wrote three python script of these models, which named `subtractor.py`, `addition-subtractor.py` and `multiply.py` to create figures, records, model savings. (You can use `model_load-subtractor.py`, `model_load-addition-subtractor.py` or `model_load-multiply.py` to reload models)\n",
    "\n",
    "# initial \n",
    "\n",
    "- In these three jupyter notebooks, I run these models in same condition.\n",
    "    - DATA_SIZE = 60000\n",
    "    - TRAIN_SIZE = 45000\n",
    "    - DIGITS = 3\n",
    "    - RNN = layers.LSTM\n",
    "    - HIDDEN_SIZE = 128\n",
    "    - BATCH_SIZE = 128\n",
    "    - LAYERS = 1\n",
    "    - EPOCH = 100\n",
    "\n",
    "## subtractor\n",
    "\n",
    "- In subtractor, the final accuracy on test dataset whose size is 15000 is `0.9802`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./fig/subtractor-jupyter-accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - You can see even growth of accuracy of training and validating was slowing down, the accuracy of testing was still growing\n",
    "    - This can prove the model was trained well and not just due to `over-fitting` train-dataset or validate-dataset.\n",
    "    - And you can see the accuracy of testing oscillate in the beginging, but after more epoches runing , it can achieve more high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](./fig/subtractor-jupyter-loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - the loss of training and validating is showed as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## addition-subtractor\n",
    "\n",
    "- In adder-subtracotr, the final accuracy on test dataset whose size is 15000 is `0.8534666666666667`\n",
    "\n",
    "![](./fig/addition-subtractor-jupyter-accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - the final accuracy on test dataset is NOT BAD\n",
    "    - As you seen in figure above, even growth of accuracy of training and validating was slowing down, the accuracy of testing was still growing.\n",
    "    - And it is apparently model can achieve more higher accuracy if it was trained after a few epoches, because the accuracy of testing was still oscillate.\n",
    "    \n",
    "![](./fig/addition-subtractor-jupyter-loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - According the loss of validating, the trained model may not found the (even local or global) optimum, it could be due to small dataset size and can be imporoved if there are more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiply\n",
    "\n",
    "- In adder-subtractor-multiplier, the final accuracy on test dataset whose size is 15000 is `0.5378666666666667`.\n",
    "\n",
    "![](./fig/multiply-jupyter-accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - The accuracy of validating is still low, which meant the model was NOT trained well, and may be due to very small dataset.\n",
    "    - the adder-subtractor-multiplier include three types of operations, and results of multiplication contained much larger digit numbers than addition or subtraction.\n",
    "\n",
    "![](./fig/multiply-jupyter-loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - The viewpoint of very small dataset can also be proved by the model loss figure as you can see the loss of vqlidating was increasing eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
